{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full name of CART is Classification and Regression Tree, which means that it can be used on both classification and regression. The regression tree is used for modeling tasks where the target variable is continuous, and its feature selection criterion uses the minimum squared error. The classification tree is used for modeling tasks where the target variable is discrete, and the feature selection criterion is the Gini index.\n",
    "\n",
    "CART is a learning method that outputs a conditional probability distribution of a random variable $Y$ given an input random variable $X$. The CART algorithm divides the input space, that is, the feature space, into finite units by selecting the optimal features and eigenvalues, and determines the predicted probability distribution on these units, that is, outputs the conditional probability distribution under the given input conditions. .\n",
    "\n",
    "The complete CART algorithm includes three parts: feature selection, decision tree generation and decision tree pruning. Whether it is a regression tree or a classification tree, the core of the algorithm is to recursively select the optimal features to build a decision tree.\n",
    "\n",
    "In addition to selecting the optimal features to build a decision tree, pruning is another important part of this algorithm. It can be regarded as a regularization method of decision tree algorithm. As a rule-based and non-parametric supervised learning method, decision tree is easy to overfit during training, resulting in low generalization performance of the final generated decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Tree\n",
    "\n",
    "Given the input variables $X$ and output variables $Y$, the generation of a regression tree correspond to the split on the input space and the output values of the divisiory units. Assuming that the input space is divided into $M$ units $R_{1}, R_{2}, \\cdots , R_{M}$, and there is a fixed output $c_{m}$ for every unit. THen the model of regression tree model can be expressed as:\n",
    "$$\n",
    "f(x)= \\sum _{m=1}^{M} c_{m}I(x \\in R_{m})\n",
    "$$\n",
    "\n",
    "When the input space partition is determined, regression tree algorithm iterates over all features and uses MSE to choose the optimal features and optimal segmentation points. \n",
    "$$\n",
    "\\min _{j . s}\\left[\\min _{c_{1}} \\sum_{x_{i} \\in R_{1}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}+\\min _{c_{2}} \\sum_{x_{i} \\in R_{2}(j, s)}\\left(y_{i}-c_{2}\\right)^{2}\\right]\n",
    "$$\n",
    "\n",
    "This method is also called Least Squares Regression Tree Algorithm. The larger the tree depth of the regression tree, the higher the model complexity and the better the fit to the data, but the corresponding generalization ability cannot be guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree\n",
    "\n",
    "CART classification tree is quite different from the regression tree but is similar with ID3 and C4.5 decision tree. Unlike ID3 and C4.5, CART classification tree uses Gini index to select features. \n",
    "\n",
    "The Gini index is for probability distributions. Assuming that there are $K$ classes in a classification problem, and the probability that the sample belongs to the $k-th$ class is $p_k$, then the Gini index of the probability distribution of the sample is\n",
    "$$\n",
    "Gini(p) = \\sum _{k=1} ^{K} p_{k}(1-p_{k})\n",
    "$$\n",
    "\n",
    "In practical classification calculation, the Gini index for a given data samples $D$ is\n",
    "$$\n",
    "Gini(D) = 1 - \\sum _{k=1} ^{K} {(\\frac{c_{k}}{D})}^{2}\n",
    "$$\n",
    "\n",
    "The corresponding conditional Gini index, that is, the Gini index of data set $D$ under the condition of given feature $A$, is calculated as follows\n",
    "$$\n",
    "Gini(D,A) = \\frac{|D_{1}|}{|D|} Gini(D_{1}) + \\frac{|D_{2}|}{|D|} Gini(D_{2})\n",
    "$$\n",
    "\n",
    "When constructing the classification tree, the feature with the smallest conditional Gini index is selected as the optimal feature to construct the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(nums):\n",
    "    probs = [nums.count(i)/len(nums) for i in set(nums)]\n",
    "    gini = sum([p*(1-p) for p in probs])\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "In order to construct a decision tree with better generalization performance, we need to prune the tree. The so-called pruning is the process of simplifying the constructed decision tree. Specifically, it is to cut some subtrees or leaf nodes from the generated tree, and use its root node or parent node as a new leaf node. Generally speaking, there are two ways of pruning, pre-pruning and post-pruning.\n",
    "\n",
    "Pre-pruning is a method of pruning in the process of tree generation. The key is to calculate whether the current feature division can improve the generalization performance of the decision tree before the nodes in the tree are expanded. If not, then the trees no longer grow. Pre-pruning is relatively straightforward, the algorithm is simple and efficient, and it is suitable for large-scale calculations, but pre-pruning may have a risk of \"early stopping\", which may lead to under-fitting of the model.\n",
    "\n",
    "Post-pruning is to wait for the tree to grow completely before pruning from the bottom leaf node. CART pruning is a post-pruning method. Simply speaking, it is to prune the complete tree node by node from the bottom up, and each pruning will form a subtree to the root node, thus forming a subtree sequence. Then perform cross-validation on all subtrees to find out which subtree has the smallest error and which is the optimal subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humility</th>\n",
       "      <th>outlook</th>\n",
       "      <th>play</th>\n",
       "      <th>temp</th>\n",
       "      <th>windy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>sunny</td>\n",
       "      <td>no</td>\n",
       "      <td>hot</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>sunny</td>\n",
       "      <td>no</td>\n",
       "      <td>hot</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>hot</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high</td>\n",
       "      <td>rainy</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td>rainy</td>\n",
       "      <td>yes</td>\n",
       "      <td>cool</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>normal</td>\n",
       "      <td>rainy</td>\n",
       "      <td>no</td>\n",
       "      <td>cool</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>normal</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>cool</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>high</td>\n",
       "      <td>sunny</td>\n",
       "      <td>no</td>\n",
       "      <td>mild</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>normal</td>\n",
       "      <td>sunny</td>\n",
       "      <td>yes</td>\n",
       "      <td>cool</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>normal</td>\n",
       "      <td>rainy</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>normal</td>\n",
       "      <td>sunny</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>high</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>normal</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>hot</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>high</td>\n",
       "      <td>rainy</td>\n",
       "      <td>no</td>\n",
       "      <td>mild</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   humility   outlook play  temp  windy\n",
       "0      high     sunny   no   hot  False\n",
       "1      high     sunny   no   hot   True\n",
       "2      high  overcast  yes   hot  False\n",
       "3      high     rainy  yes  mild  False\n",
       "4    normal     rainy  yes  cool  False\n",
       "5    normal     rainy   no  cool   True\n",
       "6    normal  overcast  yes  cool   True\n",
       "7      high     sunny   no  mild  False\n",
       "8    normal     sunny  yes  cool  False\n",
       "9    normal     rainy  yes  mild  False\n",
       "10   normal     sunny  yes  mild   True\n",
       "11     high  overcast  yes  mild   True\n",
       "12   normal  overcast  yes   hot  False\n",
       "13     high     rainy   no  mild   True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from math import log\n",
    "\n",
    "df = pd.read_csv('example_data.csv', dtype={'windy': 'str'})  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set based on feature and feature value\n",
    "def split_dataframe(data, col): \n",
    "    '''\n",
    "    input: dataframe, column name.\n",
    "    output: a dict of splited dataframe.\n",
    "    '''\n",
    "    # unique value of column\n",
    "    unique_values = data[col].unique()\n",
    "    # empty dict of dataframe\n",
    "    result_dict = {elem : pd.DataFrame for elem in unique_values}\n",
    "    # split dataframe based on column value\n",
    "    for key in result_dict.keys():\n",
    "        result_dict[key] = data[:][data[col] == key]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hot':    humility   outlook play temp  windy\n",
       " 0      high     sunny   no  hot  FALSE\n",
       " 1      high     sunny   no  hot   TRUE\n",
       " 2      high  overcast  yes  hot  FALSE\n",
       " 12   normal  overcast  yes  hot  FALSE,\n",
       " 'mild':    humility   outlook play  temp  windy\n",
       " 3      high     rainy  yes  mild  FALSE\n",
       " 7      high     sunny   no  mild  FALSE\n",
       " 9    normal     rainy  yes  mild  FALSE\n",
       " 10   normal     sunny  yes  mild   TRUE\n",
       " 11     high  overcast  yes  mild   TRUE\n",
       " 13     high     rainy   no  mild   TRUE,\n",
       " 'cool':   humility   outlook play  temp  windy\n",
       " 4   normal     rainy  yes  cool  FALSE\n",
       " 5   normal     rainy   no  cool   TRUE\n",
       " 6   normal  overcast  yes  cool   TRUE\n",
       " 8   normal     sunny  yes  cool  FALSE}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataframe(df, 'temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the best column based on Gini index\n",
    "def choose_best_col(df, label): \n",
    "    # Calculating label's gini index\n",
    "    gini_D = gini(df[label].tolist())\n",
    "    # columns list except label\n",
    "    cols = [col for col in df.columns if col not in [label]]\n",
    "    # initialize the max infomation gain, best column and best splited dict\n",
    "    min_value, best_col, min_splited = 999, None, None\n",
    "    # split data based on different column \n",
    "    for col in cols:\n",
    "        splited_set = split_dataframe(df, col) \n",
    "        gini_DA = 0\n",
    "        for subset_col, subset in splited_set.items():\n",
    "            # calculating splited dataframe label's gini index\n",
    "            gini_Di = gini(subset[label].tolist())\n",
    "            # calculating gini index of current feature\n",
    "            gini_DA += len(subset)/len(df) * gini_Di\n",
    "            if gini_DA < min_value:\n",
    "                min_value, best_col = gini_DA, col\n",
    "                min_splited = splited_set\n",
    "                \n",
    "    return min_value, best_col, min_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartTree:\n",
    "    # define a node class\n",
    "    class Node:\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.connections = {}\n",
    "\n",
    "        def connect(self, label, node):\n",
    "            self.connections[label] = node\n",
    "\n",
    "    def __init__(self, data, label):\n",
    "        self.columns = data.columns\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.root = self.Node(\"Root\")\n",
    "\n",
    "    # print the tree\n",
    "    def print_tree(self, node, tabs):\n",
    "        print(tabs + node.name)\n",
    "        for connection, child_node in node.connections.items():\n",
    "            print(tabs + \"\\t\" + \"(\" + str(connection) + \")\") \n",
    "            self.print_tree(child_node, tabs + \"\\t\\t\")\n",
    "\n",
    "    def construct_tree(self):\n",
    "        self.construct(self.root, \"\", self.data, self.columns)\n",
    "\n",
    "    # construct tree\n",
    "    def construct(self, parent_node, parent_connection_label, input_data, columns): \n",
    "        min_value, best_col, min_splited = choose_best_col(input_data[columns], self.label)\n",
    "        if not best_col:\n",
    "            node = self.Node(input_data[self.label].iloc[0]) \n",
    "            parent_node.connect(parent_connection_label, node) \n",
    "            return\n",
    "\n",
    "        node = self.Node(best_col) \n",
    "        parent_node.connect(parent_connection_label, node)\n",
    "\n",
    "        new_columns = [col for col in columns if col != best_col] \n",
    "        # Recursively constructing decision trees\n",
    "        for splited_value, splited_data in min_splited.items():\n",
    "            self.construct(node, splited_value, splited_data, new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root\n",
      "\t()\n",
      "\t\ttemp\n",
      "\t\t\t(hot)\n",
      "\t\t\t\toutlook\n",
      "\t\t\t\t\t(sunny)\n",
      "\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(FALSE)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t\t\t\t\t(TRUE)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t(overcast)\n",
      "\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(FALSE)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(FALSE)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t(mild)\n",
      "\t\t\t\toutlook\n",
      "\t\t\t\t\t(rainy)\n",
      "\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t(FALSE)\n",
      "\t\t\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t(TRUE)\n",
      "\t\t\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t(sunny)\n",
      "\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(FALSE)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(TRUE)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t(overcast)\n",
      "\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(TRUE)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t(cool)\n",
      "\t\t\t\twindy\n",
      "\t\t\t\t\t(FALSE)\n",
      "\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\toutlook\n",
      "\t\t\t\t\t\t\t\t\t(rainy)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t\t\t(sunny)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t(TRUE)\n",
      "\t\t\t\t\t\toutlook\n",
      "\t\t\t\t\t\t\t(rainy)\n",
      "\t\t\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t\t\t(overcast)\n",
      "\t\t\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n"
     ]
    }
   ],
   "source": [
    "tree = CartTree(df, 'play')\n",
    "tree.construct_tree()\n",
    "tree.print_tree(tree.root, \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77183bd9d6786d4e91446bef4a81b437a8a214642262a83c9f17e3391316dc6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
