{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Margin Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as perceptron, hard-margin support vector machine aims to find a hyperplane that separates the data into different classes. When the training data is linearly separable, there is generally more than one linear hyperplane that can classify the data. The hard-margin support vector machine uses the interval maximization to obtain an optimal separating hyperplane. A hard-margin support vector machine can be formulated as a convex quadratic programming problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min _{w, b} \\frac{1}{2}\\|w\\|^{2} \\\\\n",
    "&\\text { s.t. } \\enspace y_{i}\\left(w \\cdot x_{i}+b\\right)-1 \\geq 0, i=1,2, \\cdots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In general, we can directly solve the above convex quadratic programming, but sometimes the original problem is not easy to solve. In this case, we need to introduce Lagrangian duality to convert the original problem into a dual problem to solve. The general form of the original quadratic program is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min _{x \\in R^{n}} f(x)\\\\\n",
    "&\\text { s.t. } \\enspace c_{i}(x) \\leq 0 \\quad i=1,2 . . k\\\\\n",
    "&h_{j}(x)=0 \\quad j=1,2 . . l\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Introduce the Lagrangian function:\n",
    "$$\n",
    "L(x, \\alpha, \\beta)=f(x)+\\sum_{i=1}^{k} \\alpha_{i} c_{i}(x)+\\sum_{j=1}^{l} \\beta_{j} h_{j}(x)\n",
    "$$\n",
    "\n",
    "Define the maximization function for the function above:\n",
    "$$\n",
    "\\theta_{p}(x)=\\max _{\\alpha, \\beta, \\alpha_{i} \\geq 0}\\left(f(x)+\\sum_{i=1}^{k} \\alpha_{i} c_{i}(x)+\\sum_{j=1}^{l} \\beta_{j} h_{j}(x)\\right)\n",
    "$$\n",
    "\n",
    "The original problem is equivalent to the minimization and maximization problem of the Lagrangian function:\n",
    "$$\n",
    "\\min _{x} \\max _{\\alpha, \\beta, \\alpha_{i} \\geq 0}\\left(f(x)+\\sum_{i=1}^{k} \\alpha_{i} c_{i}(x)+\\sum_{j=1}^{l} \\beta_{j} h_{j}(x)\\right)=\\min _{x} \\theta_{p}(x)\n",
    "$$\n",
    "\n",
    "According to Lagrangian duality, the dual problem is also a maximization and minimization problem:\n",
    "$$\n",
    "\\max _{\\alpha} \\min _{w, b}L(w, b, \\alpha)\n",
    "$$\n",
    "\n",
    "To get the solution of this dual problem, we need to minimize $L(w, b, \\alpha)$ and then calculate the maximization of it for $\\alpha$. Take the partial derivative of $L(w, b, \\alpha)$ with respect to $w$ and $b$ and make it equal to 0:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\nabla_{w} L(w, b, \\alpha)=w-\\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i}=0 \\\\\n",
    "&\\nabla_{b} L(w, b, \\alpha)=\\sum_{i=1}^{N} \\alpha_{i} y_{i}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We get:\n",
    "$$\n",
    "w = \\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i} \\\\\n",
    "\\sum_{i=1}^{N} \\alpha_{i} y_{i}=0\n",
    "$$\n",
    "Substitute the result into $L(w, b, \\alpha)$ to getï¼š\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(w, b, \\alpha) &=\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} y_{i}\\left(\\left(\\sum_{j=1}^{N} \\alpha_{j} y_{j} x_{j}\\right) \\cdot x_{i}+b\\right)+\\sum_{i=1}^{N} \\alpha_{i} \\\\\n",
    "&=-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{i}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "That is:\n",
    "$$\n",
    "\\min _{w, b}L(w, b, \\alpha) = -\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{i}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i}\n",
    "$$\n",
    "\n",
    "Calculate the maximization of $L(w, b, \\alpha)$ to $\\alpha$, that is, the dual problem:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\max _{\\alpha} -\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i} \\\\\n",
    "&\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0, \\enspace \\alpha_{i} \\geqslant 0, \\enspace i=1,2, \\cdots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "According to the condition of KTT, we get:\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\nabla_{x} L\\left(x^{*}, \\alpha^{*}, \\beta^{*}\\right)=0 \\\\\n",
    "\\nabla_{\\alpha} L\\left(x^{*}, \\alpha^{*}, \\beta^{*}\\right)=0 \\\\\n",
    "\\nabla_{\\beta} L\\left(x^{*}, \\alpha^{*}, \\beta^{*}\\right)=0 \\\\\n",
    "\\alpha_{i}^{*} c_{i}\\left(x^{*}\\right)=0, \\enspace i=1,2, \\cdots, k \\\\\n",
    "c_{i}\\left(x^{*}\\right) \\leq 0, \\enspace i=1,2, \\cdots, k \\\\\n",
    "\\alpha_{i}^{*} \\geq 0, \\enspace i=1,2, \\cdots, k \\\\\n",
    "h_{j}\\left(x^{*}\\right)=0, \\enspace j=1,2, \\cdots, l\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Finally, the solution of the original problem can be obtained according to the dual problem as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w^{*} &=\\sum_{i=1}^{N} \\alpha_{i}^{*} y_{i} x_{i} \\\\\n",
    "b^{*} &=y_{j}-\\sum_{i=1}^{N} \\alpha_{i}^{*} y_{i}\\left(x_{i} \\cdot x_{j}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dict = {-1:np.array([[1,7], [2,8], [3,8],]), 1:np.array([[5,1], [6,-1], [7,3],])}\n",
    "\n",
    "colors = {1:'r',-1:'g'}\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "[[ax.scatter(x[0], x[1], s=100, color=colors[i]) for x in data_dict[i]] for i in data_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    # parameter dictionary\n",
    "    opt_dict = {}\n",
    "    \n",
    "    # data transformation list\n",
    "    transforms = [[1,1], [-1,1], [-1,-1], [1,-1]]\n",
    "    \n",
    "    # obtain data from the dictionary\n",
    "    all_data = []\n",
    "    for yi in data:\n",
    "        for featureset in data[yi]: \n",
    "            for feature in featureset:\n",
    "                all_data.append(feature)\n",
    "    \n",
    "    # obtain the max and min value of data\n",
    "    max_feature_value = max(all_data) \n",
    "    min_feature_value = min(all_data) \n",
    "    all_data = None\n",
    "    \n",
    "    # define a list of step sizes\n",
    "    step_sizes = [max_feature_value * 0.1, max_feature_value * 0.01, max_feature_value * 0.001 ]\n",
    "    \n",
    "    # set up the range of parameter b\n",
    "    b_range_multiple = 2\n",
    "    b_multiple = 5\n",
    "    latest_optimum = max_feature_value * 10\n",
    "\n",
    "    # optimization based on different step size training\n",
    "    for step in step_sizes:\n",
    "        w = np.array([latest_optimum, latest_optimum])\n",
    "        # convex optimization\n",
    "        optimized = False\n",
    "        while not optimized:\n",
    "            for b in np.arange(-1*(max_feature_value*b_range_multiple), max_feature_value*b_range_multiple, step*b_multiple):\n",
    "                for transformation in transforms:\n",
    "                    w_t = w*transformation \n",
    "                    found_option = True\n",
    "\n",
    "                    for i in data:\n",
    "                        for xi in data[i]:\n",
    "                            yi=i\n",
    "                            if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
    "                                found_option = False\n",
    "\n",
    "                    if found_option:\n",
    "                        opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
    "\n",
    "            if w[0] < 0:\n",
    "                optimized = True\n",
    "                print('Optimized a step!') \n",
    "            else:\n",
    "                w = w - step\n",
    "\n",
    "        norms = sorted([n for n in opt_dict])\n",
    "        opt_choice = opt_dict[norms[0]]\n",
    "        w = opt_choice[0]\n",
    "        b = opt_choice[1]\n",
    "        latest_optimum = opt_choice[0][0]+step*2\n",
    "\n",
    "    for i in data:\n",
    "        for xi in data[i]:\n",
    "            yi=i\n",
    "            print(xi,':',yi*(np.dot(w,xi)+b)) \n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized a step!\n",
      "Optimized a step!\n",
      "Optimized a step!\n",
      "[1 7] : 1.271999999999435\n",
      "[2 8] : 1.271999999999435\n",
      "[3 8] : 1.0399999999995864\n",
      "[5 1] : 1.0479999999990506\n",
      "[ 6 -1] : 1.7439999999985962\n",
      "[7 3] : 1.0479999999990506\n"
     ]
    }
   ],
   "source": [
    "w, b = train(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction function\n",
    "def predict(features):\n",
    "    classification = np.sign(np.dot(np.array(features),w)+b)\n",
    "    if classification != 0:\n",
    "        ax.scatter(features[0], features[1], s=200, marker='^', c=colors[classification])\n",
    "    print(classification)\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "predict_us = [[0, 10], [1, 3], [3, 4], [5, 6], [6, -5], [2, 5], [8, -3]]\n",
    "\n",
    "for p in predict_us:\n",
    "    predict(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77183bd9d6786d4e91446bef4a81b437a8a214642262a83c9f17e3391316dc6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
