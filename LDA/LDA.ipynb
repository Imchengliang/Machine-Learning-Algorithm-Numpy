{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (linear discriminant analysis) is a classic linear classification method. Different from the idea of maximizing variance in PCA dimensionality reduction, the basic idea of LDA is to project the data into a low-dimensional space, so that the same type of data is as close as possible, and different types of data are as far apart as possible. So, LDA is a supervised linear classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data set $D=\\{(x_{i}, y{i}) \\}^{m}_{i=1}, \\enspace y_{i} \\in \\{0,1\\}$, and $X_{i}, \\enspace \\mu _{i}, \\enspace \\Sigma _{i}$ are the set, mean vector and covariance matrix of the $i$-th class of data.\n",
    "\n",
    "Assuming that the above data is projected onto the straight line $w$, the projections of the centers of the two types of samples onto the straight line are $w^{\\top}\\mu _{0}$ and $w^{\\top}\\mu _{1}$, respectively. Considering all the sample projections, the covariances of the two types of samples are $w^{\\top}\\Sigma _{0} w$ and $w^{\\top}\\Sigma _{1} w$, respectively. Since the straight line $w$ is a one-dimensional space, the above values are all real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization goals are to make the projection points of similar samples as close as possible, and the projection points of heterogeneous samples as far away as possible. \n",
    "\n",
    "To meet the first purpose, the covariance of the projection points of the similar sample need to be as small as possible, that is, $w^{\\top}\\Sigma _{0} w+w^{\\top}\\Sigma _{1} w$ as small as possible. The projection points of heterogeneous samples to be as far away as possible, which means the distance between the class center points to be as large as possible, that is, $||w^{\\top}\\mu _{0}-w^{\\top}\\mu _{1}||^{2}_{2}$ is as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering these two optimization objectives at the same time, the objective function can be defined as:\n",
    "$$\n",
    "J=\\frac{\\left\\|\\boldsymbol{w}^{T} \\boldsymbol{\\mu}_{0}-\\boldsymbol{w}^{T} \\boldsymbol{\\mu}_{1}\\right\\|_{2}^{2}}{\\boldsymbol{w}^{T} \\boldsymbol{\\Sigma}_{0} \\boldsymbol{w}+\\boldsymbol{w}^{T} \\boldsymbol{\\Sigma}_{1} \\boldsymbol{w}}=\\frac{\\boldsymbol{w}^{T}\\left(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1}\\right)\\left(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1}\\right)^{T} \\boldsymbol{w}}{\\boldsymbol{w}^{T}\\left(\\Sigma_{0}+\\Sigma_{1}\\right) \\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Define intra-class divergence matrix $S_w$ and between-class divergence matrix $S_b$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&S_{w}=\\Sigma _{0} + \\Sigma _{1} \\\\\n",
    "&S_{b}=(\\mu _{0}-\\mu _{1})(\\mu _{0}-\\mu _{1})^{\\top}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then the objective function can be rewritten as:\n",
    "$$\n",
    "J = \\frac{w^{\\top}S_{b}w}{w^{\\top}S_{w}w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formula is the ultimate optimization goal of LDA. To calculate $w$, $w^{\\top}S_{w}w$can be set to be $1$, then the optimization formula can be expressed as:\n",
    "$$\n",
    "\\min -w^{\\top}S_{b}w \\\\\n",
    "s.t. \\enspace w^{\\top}S_{w}w = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Lagrange Multiplier Method for optimizing solution based on conditional constraints, the above formula is equal to:\n",
    "$$\n",
    "S_{b}w = \\lambda S_{w}w\n",
    "$$\n",
    "\n",
    "Let $S_{b}w = \\lambda (\\mu _{0} - \\mu _{1})$, and put it into the above formula:\n",
    "$$\n",
    "w = S^{-1}_{w}(\\mu _{0} - \\mu _{1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the stability of the numerical solution of matrix $S_w$, singular value decomposition can be performed on it, that is:\n",
    "$$\n",
    "S_{w} = U\\Sigma V^{\\top}\n",
    "$$\n",
    "\n",
    "Finally, take its inverse to get $S_{w}^{-1}$, and use it to calculate $w$. The projected data points are $Y=S_{w}^{\\top}X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    def __init__(self):\n",
    "        # initialize weight matrix\n",
    "        self.w = None\n",
    "\n",
    "    def calc_cov(self, X, Y=None):\n",
    "        m = X.shape[0]\n",
    "        # standardization\n",
    "        X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "        Y = X if Y == None else (Y - np.mean(Y, axis=0))/np.std(Y, axis=0)\n",
    "        return 1 / m * np.matmul(X.T, Y)\n",
    "\n",
    "    def project(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        X_projection = X.dot(self.w)\n",
    "        return X_projection\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X0 = X[y == 0]\n",
    "        X1 = X[y == 1]\n",
    "\n",
    "        sigma0 = self.calc_cov(X0)\n",
    "        sigma1 = self.calc_cov(X1)\n",
    "\n",
    "        Sw = sigma0 + sigma1\n",
    "\n",
    "        u0, u1 = np.mean(X0, axis=0), np.mean(X1, axis=0)\n",
    "        mean_diff = np.atleast_1d(u0 - u1)\n",
    "\n",
    "        U, S, V = np.linalg.svd(Sw)\n",
    "        Sw_ = np.dot(np.dot(V.T, np.linalg.pinv(np.diag(S))), U.T)\n",
    "        self.w = Sw_.dot(mean_diff)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for sample in X:\n",
    "            h = sample.dot(self.w)\n",
    "            y = 1 * (h < 0)\n",
    "            y_pred.append(y)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 4) (20, 4) (80,) (20,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "class Plot():\n",
    "    def __init__(self): \n",
    "        self.cmap = plt.get_cmap('viridis')\n",
    "\n",
    "    def calculate_covariance_matrix(self, X, Y=None):\n",
    "        m = X.shape[0]\n",
    "        # standardization\n",
    "        X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "        Y = X if Y == None else (Y - np.mean(Y, axis=0))/np.std(Y, axis=0)\n",
    "        return 1 / m * np.matmul(X.T, Y)\n",
    "\n",
    "    def _transform(self, X, dim):\n",
    "        covariance = self.calculate_covariance_matrix(X)\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
    "        # Sort eigenvalues and eigenvector by largest eigenvalues\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues = eigenvalues[idx][:dim]\n",
    "        eigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :dim]\n",
    "        # Project the data onto principal components\n",
    "        X_transformed = X.dot(eigenvectors)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def plot_regression(self, lines, title, axis_labels=None, mse=None, scatter=None, legend={\"type\": \"lines\", \"loc\": \"lower right\"}):\n",
    "        \n",
    "        if scatter:\n",
    "            scatter_plots = scatter_labels = []\n",
    "            for s in scatter:\n",
    "                scatter_plots += [plt.scatter(s[\"x\"], s[\"y\"], color=s[\"color\"], s=s[\"size\"])]\n",
    "                scatter_labels += [s[\"label\"]]\n",
    "            scatter_plots = tuple(scatter_plots)\n",
    "            scatter_labels = tuple(scatter_labels)\n",
    "\n",
    "        for l in lines:\n",
    "            li = plt.plot(l[\"x\"], l[\"y\"], color=s[\"color\"], linewidth=l[\"width\"], label=l[\"label\"])\n",
    "\n",
    "        if mse:\n",
    "            plt.suptitle(title)\n",
    "            plt.title(\"MSE: %.2f\" % mse, fontsize=10)\n",
    "        else:\n",
    "            plt.title(title)\n",
    "\n",
    "        if axis_labels:\n",
    "            plt.xlabel(axis_labels[\"x\"])\n",
    "            plt.ylabel(axis_labels[\"y\"])\n",
    "\n",
    "        if legend[\"type\"] == \"lines\":\n",
    "            plt.legend(loc=\"lower_left\")\n",
    "        elif legend[\"type\"] == \"scatter\" and scatter:\n",
    "            plt.legend(scatter_plots, scatter_labels, loc=legend[\"loc\"])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the dataset X and the corresponding labels y in 2D using PCA.\n",
    "    def plot_in_2d(self, X, y=None, title=None, accuracy=None, legend_labels=None):\n",
    "        X_transformed = self._transform(X, dim=2)\n",
    "        x1 = X_transformed[:, 0]\n",
    "        x2 = X_transformed[:, 1]\n",
    "        class_distr = []\n",
    "\n",
    "        y = np.array(y).astype(int)\n",
    "\n",
    "        colors = [self.cmap(i) for i in np.linspace(0, 1, len(np.unique(y)))]\n",
    "\n",
    "        # Plot the different class distributions\n",
    "        for i, l in enumerate(np.unique(y)):\n",
    "            _x1 = x1[y == l]\n",
    "            _x2 = x2[y == l]\n",
    "            _y = y[y == l]\n",
    "            class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))\n",
    "\n",
    "        # Plot legend\n",
    "        if not legend_labels is None: \n",
    "            plt.legend(class_distr, legend_labels, loc=1)\n",
    "\n",
    "        # Plot title\n",
    "        if title:\n",
    "            if accuracy:\n",
    "                perc = 100 * accuracy\n",
    "                plt.suptitle(title)\n",
    "                plt.title(\"Accuracy: %.1f%%\" % perc, fontsize=10)\n",
    "            else:\n",
    "                plt.title(title)\n",
    "\n",
    "        # Axis labels\n",
    "        plt.xlabel('class 1')\n",
    "        plt.ylabel('class 2')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Plot().plot_in_2d(X_test, y_pred, title=\"LDA\", accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
