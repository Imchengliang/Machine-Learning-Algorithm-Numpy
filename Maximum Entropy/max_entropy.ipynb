{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Maximum Entropy Principle is a general principle based on the information entropy theory. The classification model based on the principle of maximum entropy is also called the maximum entropy model. Information entropy is a quantity that describes the uncertainty of information. The maximum entropy method considers that the maximum probability distribution of entropy under the constraints obtained from known information is a probability distribution that makes full use of known information and makes the least assumptions about unknown parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a discrete random variable $x$, its information entropy can be defined as:\n",
    "$$\n",
    "H = -\\sum^{n}_{i=1}f(x_{i})\\ln f(x_{i})\n",
    "$$\n",
    "\n",
    "For a continuous random variable $x$, its information entropy can be defined as:\n",
    "$$\n",
    "H=-\\int_{R} f(x) \\ln f(x) d x\n",
    "$$\n",
    "\n",
    "$f(x)$ is the probability density function of the distribution function, and $f(x_{i})$ is the probability distribution of discrete points. The maximum entropy method is to obtain $f(x)$ or $f(x_{i})$ under the given constraints so that the entropy $H$ reaches the maximum value, which is essentially an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the target classification model is a condition probability distribution $P(Y|X)$, $X$ represents input and $Y$ represents output. With the given training data set, the learning goal is to select the maximum entropy model as the target model. And with the data set, the empirical distribution $\\hat{P}(X,Y)$ of joint probability distribution $P(X,Y)$ and the empirical distribution $\\hat{P}(X)$ of marginal probability distribution $P(X)$ can be determined. Then characteristic function $f(x,y)$ can be used to describe the fact between input and output. When $x$ and $y$ satisfy a fact, the function takes the value of 1, otherwise, it takes the value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of the characteristic function $f(x,y)$ with respect to the empirical distribution $\\hat{P}(X,Y)$ is $E_{\\hat{P}}(f)$:\n",
    "$$\n",
    "E_{\\hat{P}}(f) = \\sum _{x,y} \\hat{P}(X,Y)f(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of the characteristic function $f(x,y)$ with respect to the empirical distribution $\\hat{P}(X)$ is $E_{P}(f)$:\n",
    "$$\n",
    "E_{P}(f) = \\sum _{x,y} \\hat{P}(X)P(y|x)f(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model can obtain enough information from the known data, it can be assumed that the above two expected values are equal:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum _{x,y} \\hat{P}(X,Y)f(x,y) &= \\sum _{x,y} \\hat{P}(X)P(y|x)f(x,y) \\\\\n",
    "E_{\\hat{P}}(f) &= E_{P}(f)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formula can be used as a constraint condition for the maximum entropy model learning. If there are $n$ characteristic functions, there are $n$ constraints.\n",
    "\n",
    "Assuming that the model set that satisfies the above constraints is $C$, the model with the largest conditional entropy defined in the model set is the maximum entropy model:\n",
    "$$\n",
    "\\max_{P \\in C}H(P) = -\\sum_{x,y} \\hat{P}(x)P(y|x) \\log P(y|x) \\\\\n",
    "s.t. \\enspace E_{\\hat{P}}(f) = E_{P}(f), \\enspace \\sum_{y} P(y|x)=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the above maximization problem as a minimization problem:\n",
    "$$\n",
    "\\min_{P \\in C} -H(P) = \\sum_{x,y} \\hat{P}(x)P(y|x) \\log P(y|x) \\\\\n",
    "s.t. \\enspace E_{\\hat{P}}(f) - E_{P}(f)=0, \\enspace \\sum_{y} P(y|x)=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above constrained optimization can be transformed into an unconstrained optimization problem by the Lagrangian multiplier method, and its original problem can be transformed into a dual problem to solve, and the Lagrangian function $L(P,W)$ is defined as follow:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(P, W)&=-H(P)+w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right)+\\sum_{i=1}^{n} w_{i}\\left(E_{\\hat{P}}\\left(f_{i}\\right)-E_{P}\\left(f_{i}\\right)\\right) \\\\\n",
    "&=\\sum_{x, y} \\hat{P}(x) P(y \\mid x) \\log P(y \\mid x)+w_{0}\\left(1-\\sum_{y} P(y \\mid x)\\right) \\\\\n",
    "&+\\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\hat{P}(X, Y) f_{i}(x, y)-\\sum_{x, y} \\hat{P}(X) P(y \\mid x) f_{i}(x, y)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original optimization problem is:\n",
    "$$\n",
    "\\min_{P \\in C} \\max_{w} L(P, w)\n",
    "$$\n",
    "\n",
    "The dual problem of the problem above is:\n",
    "$$\n",
    "\\max_{w} \\min_{P \\in C} L(P, w)\n",
    "$$\n",
    "\n",
    "To solve the dual problem, first to solve the internal minimization problem $\\min _{P \\in C} L(P, w)$:\n",
    "$$\n",
    "\\Psi(w)=\\min _{P \\in C} L(P, w)=L\\left(P_{w}, w\\right) \\\\\n",
    "P_{w}=\\arg \\min _{P \\in C} L(P, w)=P_{w}(y \\mid x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the partial derivative of $L(P,w)$ with respect to $P(y|x)$ and set it to 0, the following can be obtained:\n",
    "$$\n",
    "P_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n",
    "Z_{w}(x) = \\sum _{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n",
    "$$\n",
    "\n",
    "The model represented by the  formula of $P_{w}(y \\mid x)$ is maximum entropy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then solve the external maximization problem $\\max_{w} \\Psi (w)$ and mark it as $w^{*}$:\n",
    "$$\n",
    "w^{*} = \\arg \\max_{w} \\Psi (w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum entropy model can be attributed to the maximization of the dual function $\\Psi (w)$, and $P^{*}=P_{w^{*}}=P_{w^{*}}(y|x)$  obtained by the optimization solution is the final maximum entropy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt:\n",
    "    def __init__(self, max_iter=100):\n",
    "        # input\n",
    "        self.X_ = None\n",
    "        # output: label\n",
    "        self.y_ = None\n",
    "        # number of label\n",
    "        self.m = None   \n",
    "        # number of feature\n",
    "        self.n = None   \n",
    "        # number of training sample\n",
    "        self.N = None   \n",
    "        # constant characteristic value\n",
    "        self.M = None\n",
    "        # weights\n",
    "        self.w = None\n",
    "        # label name\n",
    "        self.labels = defaultdict(int)\n",
    "        # feature name\n",
    "        self.features = defaultdict(int)\n",
    "        # max iteration number\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    # expectation of feature function with respect of empirical joint distribution P(X,Y)\n",
    "    def _EP_hat_f(self, x, y):\n",
    "        self.Pxy = np.zeros((self.m, self.n))\n",
    "        self.Px = np.zeros(self.n)\n",
    "        for x_, y_ in zip(x, y):\n",
    "            # traverse every sample\n",
    "            for x__ in set(x_):\n",
    "                self.Pxy[self.labels[y_], self.features[x__]] += 1\n",
    "                self.Px[self.features[x__]] += 1           \n",
    "        self.EP_hat_f = self.Pxy/self.N\n",
    "    \n",
    "    # expectation of feature function with respect of model P(Y|X) and empirical distribution P(X) \n",
    "    def _EP_f(self):\n",
    "        self.EP_f = np.zeros((self.m, self.n))\n",
    "        for X in self.X_:\n",
    "            pw = self._pw(X)\n",
    "            pw = pw.reshape(self.m, 1)\n",
    "            px = self.Px.reshape(1, self.n)\n",
    "            self.EP_f += pw*px / self.N\n",
    "    \n",
    "    ### maximum entropy model P(y|x)\n",
    "    def _pw(self, x):\n",
    "        mask = np.zeros(self.n+1)\n",
    "        for ix in x:\n",
    "            mask[self.features[ix]] = 1\n",
    "        tmp = self.w * mask[1:]\n",
    "        pw = np.exp(np.sum(tmp, axis=1))\n",
    "        Z = np.sum(pw)\n",
    "        pw = pw/Z\n",
    "        return pw\n",
    "\n",
    "    # improved iterative scaling method based on IIS\n",
    "    def fit(self, x, y):\n",
    "        self.X_ = x\n",
    "        self.y_ = list(set(y))\n",
    "        # input data flattened collection\n",
    "        tmp = set(self.X_.flatten())\n",
    "        self.features = defaultdict(int, zip(tmp, range(1, len(tmp)+1)))   \n",
    "        self.labels = dict(zip(self.y_, range(len(self.y_))))\n",
    "        self.n = len(self.features)+1  \n",
    "        self.m = len(self.labels)\n",
    "        self.N = len(x)  \n",
    "        # calculate EP_hat_f\n",
    "        self._EP_hat_f(x, y)\n",
    "        # initialize coefficient matrix\n",
    "        self.w = np.zeros((self.m, self.n))\n",
    "        # loop iteration\n",
    "        i = 0\n",
    "        while i <= self.max_iter:\n",
    "            # calculate EPf\n",
    "            self._EP_f()\n",
    "            self.M = 100\n",
    "            # IIS step(3)\n",
    "            tmp = np.true_divide(self.EP_hat_f, self.EP_f)\n",
    "            tmp[tmp == np.inf] = 0\n",
    "            tmp = np.nan_to_num(tmp)\n",
    "            sigma = np.where(tmp != 0, 1/self.M*np.log(tmp), 0)  \n",
    "            # IIS step(4)\n",
    "            self.w = self.w + sigma\n",
    "            i += 1\n",
    "        print('training done.')\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        res = np.zeros(len(x), dtype=np.int64)\n",
    "        for ix, x_ in enumerate(x):\n",
    "            tmp = self._pw(x_)\n",
    "            print(tmp, np.argmax(tmp), self.labels)\n",
    "            res[ix] = self.labels[self.y_[np.argmax(tmp)]]\n",
    "        return np.array([self.y_[ix] for ix in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (105,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "raw_data = load_iris()\n",
    "X, labels = raw_data.data, raw_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=43)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done.\n",
      "[0.87116843 0.04683368 0.08199789] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.00261138 0.49573305 0.50165557] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.12626693 0.017157   0.85657607] 2 {0: 0, 1: 1, 2: 2}\n",
      "[1.55221378e-04 4.45985560e-05 9.99800180e-01] 2 {0: 0, 1: 1, 2: 2}\n",
      "[7.29970746e-03 9.92687370e-01 1.29226740e-05] 1 {0: 0, 1: 1, 2: 2}\n",
      "[0.01343943 0.01247887 0.9740817 ] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.85166079 0.05241898 0.09592023] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.00371481 0.00896982 0.98731537] 2 {0: 0, 1: 1, 2: 2}\n",
      "[2.69340079e-04 9.78392776e-01 2.13378835e-02] 1 {0: 0, 1: 1, 2: 2}\n",
      "[0.01224702 0.02294254 0.96481044] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.00323508 0.98724246 0.00952246] 1 {0: 0, 1: 1, 2: 2}\n",
      "[0.00196548 0.01681989 0.98121463] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.00480966 0.00345107 0.99173927] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.00221101 0.01888735 0.97890163] 2 {0: 0, 1: 1, 2: 2}\n",
      "[9.87528545e-01 3.25313387e-04 1.21461416e-02] 0 {0: 0, 1: 1, 2: 2}\n",
      "[3.84153917e-05 5.25603786e-01 4.74357798e-01] 1 {0: 0, 1: 1, 2: 2}\n",
      "[0.91969448 0.00730851 0.07299701] 0 {0: 0, 1: 1, 2: 2}\n",
      "[3.48493252e-03 9.96377722e-01 1.37345863e-04] 1 {0: 0, 1: 1, 2: 2}\n",
      "[0.00597935 0.02540794 0.96861271] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.96593729 0.01606867 0.01799404] 0 {0: 0, 1: 1, 2: 2}\n",
      "[7.07324443e-01 2.92672257e-01 3.29961259e-06] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.96122092 0.03604362 0.00273547] 0 {0: 0, 1: 1, 2: 2}\n",
      "[9.92671813e-01 7.31265179e-03 1.55352641e-05] 0 {0: 0, 1: 1, 2: 2}\n",
      "[9.99997290e-01 2.58555077e-06 1.24081335e-07] 0 {0: 0, 1: 1, 2: 2}\n",
      "[1.77991802e-05 4.62006560e-04 9.99520194e-01] 2 {0: 0, 1: 1, 2: 2}\n",
      "[9.99995176e-01 3.85240188e-06 9.72067357e-07] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.15306343 0.21405142 0.63288515] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.25817329 0.28818997 0.45363674] 2 {0: 0, 1: 1, 2: 2}\n",
      "[2.43530473e-04 4.07929999e-01 5.91826471e-01] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.71160155 0.27290911 0.01548934] 0 {0: 0, 1: 1, 2: 2}\n",
      "[2.94976826e-06 2.51510534e-02 9.74845997e-01] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.97629163 0.00331591 0.02039245] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.04513811 0.01484173 0.94002015] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.61382753 0.38321073 0.00296174] 0 {0: 0, 1: 1, 2: 2}\n",
      "[9.65538451e-01 3.86322918e-06 3.44576854e-02] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.00924088 0.01731108 0.97344804] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.02511142 0.93818613 0.03670245] 1 {0: 0, 1: 1, 2: 2}\n",
      "[9.99127831e-01 3.29723254e-04 5.42445518e-04] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.05081665 0.0038204  0.94536295] 2 {0: 0, 1: 1, 2: 2}\n",
      "[9.99985376e-01 6.85280694e-06 7.77081022e-06] 0 {0: 0, 1: 1, 2: 2}\n",
      "[9.99791732e-01 2.06536005e-04 1.73191035e-06] 0 {0: 0, 1: 1, 2: 2}\n",
      "[2.72323181e-04 2.99692548e-03 9.96730751e-01] 2 {0: 0, 1: 1, 2: 2}\n",
      "[0.02005139 0.97151852 0.00843009] 1 {0: 0, 1: 1, 2: 2}\n",
      "[0.95642409 0.02485912 0.01871679] 0 {0: 0, 1: 1, 2: 2}\n",
      "[0.00297317 0.01261126 0.98441558] 2 {0: 0, 1: 1, 2: 2}\n",
      "0.37777777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/9sw7l7t1625_t5p1j53brwx80000gn/T/ipykernel_55486/842658133.py:86: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tmp = np.true_divide(self.EP_hat_f, self.EP_f)\n",
      "/var/folders/2m/9sw7l7t1625_t5p1j53brwx80000gn/T/ipykernel_55486/842658133.py:89: RuntimeWarning: divide by zero encountered in log\n",
      "  sigma = np.where(tmp != 0, 1/self.M*np.log(tmp), 0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "maxent = MaxEnt()\n",
    "maxent.fit(X_train, y_train)\n",
    "y_pred = maxent.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsamplespace = np.arange(6) + 1\\nmodel = maxentropy.Model(samplespace)\\nmodel.verbose = True\\n# set the expectation value\\nK=[4.5]\\nmodel.fit(f, K)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum entropy model in maxentropy\n",
    "import maxentropy\n",
    "'''\n",
    "samplespace = np.arange(6) + 1\n",
    "model = maxentropy.Model(samplespace)\n",
    "model.verbose = True\n",
    "# set the expectation value\n",
    "K=[4.5]\n",
    "model.fit(f, K)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
