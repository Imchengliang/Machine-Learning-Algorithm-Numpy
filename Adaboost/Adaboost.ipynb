{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full name of Adaboost is Adaptive boosting, which is a process of learning multiple weak classifiers and performing linear combinations on them by changing the weights of training samples.\n",
    "\n",
    "Generally speaking, there are two problems that all boosting methods have to face. One is how to change the weight or probability distribution of training samples during the training process, and the other is how to combine multiple weak classifiers into a strong classifier. \n",
    "\n",
    "In response to these two problems, Adaboost's solution is very simple. The first is to increase the weight of the samples that were misclassified by the weak classifier in the previous round, and reduce the weight of the correctly classified samples. The second is to linearly combine multiple weak classifiers to increase the weight of weak classifiers with good classification effect and reduce the weight of weak classifiers with large classification error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a binary training data set $T=\\{ (x_{1},y_{1}), (x_{2},y_{2}), \\cdots , (x_{N},y_{N}) \\}$ Each sample consists of input instances and corresponding labels: instance $x_{i} \\in \\chi \\subseteq R^{n}$, label $y_{i} \\in y \\subseteq \\{-1,1\\}$. The algorithm flow is as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Initialize the weight distribution of the training samples, assuming that each sample has the same weight at the beginning of training, that is, the sample weights are uniformly distributed. $D_{1} = (w_{11}, \\cdots, w_{1i}, \\cdots, w_{1N}), w_{1i}=\\frac{1}{N}, i=1,2, \\cdots,N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) For $m=1,2, \\cdots, M$\n",
    "\n",
    "Use the data set with initialized-uniform distribution $D_{m}$ to train to obtain a weak classifier $G_{m}(x): \\chi \\rightarrow \\{-1, 1\\}$\n",
    "\n",
    "Calculate the classification error rate of $G_{m}(x)$ on training data: $e_{m}=P\\left(G_{m}\\left(x_{i} \\neq y_{i}\\right)\\right)=\\sum_{i=1}^{N} w_{m i} I\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right)$\n",
    "\n",
    "Calculate the weight of weak classifier: $\\alpha _{m}=\\frac{1}{2} \\log \\frac{1-e_{m}}{e_{m}} $\n",
    "\n",
    "Update the weight distribution on training samples: $D_{m+1}=\\left(w_{m+1,1}, \\ldots w_{m+1, i}, \\ldots w_{m+1, N}\\right), w_{m+1, i}=\\frac{w_{m i}}{Z_{m}} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right)$, $Z_{m}$ is the  normalization factor $Z_{m}=\\sum_{i=1}^{N} w_{m i} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Construct the linear combination of multiple weak classifier: $f(x)=\\sum^{M}_{i=1} \\alpha_{m}G_{m}(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model of Adaboost classifier can be expressed as:\n",
    "$$\n",
    "G(x) = sign(f(x)) = sign(\\sum^{M}_{i=1} \\alpha_{m}G_{m}(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Adaboost is very simple but this algorithm is efficient in practice. Adaboost usually uses decision stump as the weak classifier, which is very simple and flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # determine the label of a sample is 1 or -1 \n",
    "        # based on threshold\n",
    "        self.polarity = 1\n",
    "        self.feature_index = None\n",
    "        self.threshpld = None\n",
    "        # accuracy of classification\n",
    "        self.alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, n_estimators=5):\n",
    "        # number of weak classifier\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        # step(1): initialize the uniform distribution of weights\n",
    "        w = np.full(m, (1/m))\n",
    "        # initialize the classifier list\n",
    "        self.estimators = []\n",
    "        # step(2)\n",
    "        for _ in range(self.n_estimators):\n",
    "            # 2.a: train a weak classifier\n",
    "            estimator = DecisionStump()\n",
    "            min_error = float('inf')\n",
    "            # traverse the features and select the best splitted feature \n",
    "            # based on the smallest classification error rate\n",
    "            for i in range(n):\n",
    "                # obtain the feature values\n",
    "                values = np.expand_dims(X[:, i], axis=1)\n",
    "                unique_values = np.unique(values)\n",
    "                # try every feature value as threshold\n",
    "                for threshold in unique_values:\n",
    "                    p = 1\n",
    "                    # initialize all predicted value to be 1\n",
    "                    pred = np.ones(np.shape(y))\n",
    "                    # set the predicted value (smaller than threshold) to be -1\n",
    "                    pred[X[:, i] < threshold] = -1\n",
    "                    # 2.b: calculate misclassification rate\n",
    "                    error = sum(w[y != pred])\n",
    "\n",
    "                    # If the classification error rate is greater than 0.5, \n",
    "                    # the positive and negative prediction flip is performed\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # save the parameter once the smallest classification error rate is found\n",
    "                    if error < min_error:\n",
    "                        estimator.label = p\n",
    "                        estimator.threshold = threshold\n",
    "                        estimator.feature_index = i\n",
    "                        min_error = error\n",
    "\n",
    "            # 2.c: calculate the weight of base classifier\n",
    "            estimator.alpha = 0.5 * np.log((1.0 - min_error) / (min_error + 1e-9))\n",
    "            # initialize all predicted values to be 1\n",
    "            preds = np.ones(np.shape(y))\n",
    "            # obtain the negative index that is smaller than threshold\n",
    "            negative_idx = (estimator.label * X[:, estimator.feature_index] < estimator.label * estimator.threshold)\n",
    "            # set the negative class to be -1\n",
    "            preds[negative_idx] = -1\n",
    "            # 2.d: update the sample weight\n",
    "            w *= np.exp(-estimator.alpha * y * preds)\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # save the weak classifier\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        m = len(X)\n",
    "        y_pred = np.zeros((m, 1))\n",
    "        # calculate the predicted value for each weak classifier\n",
    "        for estimator in self.estimators:\n",
    "            predictions = np.ones(np.shape(y_pred))\n",
    "            negative_idx = (estimator.label * X[:, estimator.feature_index] < estimator.label * estimator.threshold)\n",
    "            predictions[negative_idx] = -1\n",
    "            # 2.e: the prediction results of each weak classifier are weighted\n",
    "            y_pred += estimator.alpha * predictions\n",
    "\n",
    "        # return the final result\n",
    "        y_pred = np.sign(y_pred).flatten()\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost by numpy: 0.884\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = datasets.load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "digit1 = 1\n",
    "digit2 = 8\n",
    "idx = np.append(np.where(y==digit1)[0], np.where(y==digit2)[0])\n",
    "y = data.target[idx]\n",
    "# Change labels to {-1, 1}\n",
    "y[y == digit1] = -1\n",
    "y[y == digit2] = 1\n",
    "X = data.data[idx]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)\n",
    "\n",
    "clf = Adaboost(n_estimators=5)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of AdaBoost by numpy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost by sklearn: 0.924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf_ = AdaBoostClassifier(n_estimators=5, random_state=0)\n",
    "clf_.fit(X_train, y_train)\n",
    "y_pred_ = clf_.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_)\n",
    "print(\"Accuracy of AdaBoost by sklearn:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
