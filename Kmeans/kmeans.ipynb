{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Analysis is a classic unsupervised learning algorithm. With given samples, cluster analysis divides the data into several categories based on measurement method of feature similarity or distance. Commonly used cluster analysis methods include hierarchical clustering, k-means clustering, fuzzy clustering, and density clustering.\n",
    "\n",
    "Similarity measure or distance measure is the key of cluster analysis. The following are some frequently used  measure method of distance and similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Minkowski Distance\n",
    "\n",
    "Given a set of $m$-dimensional vector samples X, $x_{i},x_{j} \\in X, \\enspace x_{i}=(x_{1i}, x_{2i}, \\cdots, x_{mi})^{\\top}, \\enspace x_{j}=(x_{1j}, x_{2j}, \\cdots, x_{mj})^{\\top} $, the Minkowski Distance between $x_{i}$ and $x_{j}$ can be defined as:\n",
    "$$\n",
    "d_{ij} = (\\sum^{m}_{k=1}|x_{ki}-x_{kj}|^{p})^{\\frac{1}{p}}, \\enspace p \\ge 1\n",
    "$$\n",
    "\n",
    "When $p=2$, Minkowski Distance can be called as Euclidean Distance:\n",
    "$$\n",
    "d_{ij} = (\\sum^{m}_{k=1}|x_{ki}-x_{kj}|^{2})^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "When $p=1$, Minkowski Distance is Manhatan Distance:\n",
    "$$\n",
    "d_{ij} = \\sum^{m}_{k=1}|x_{ki}-x_{kj}|\n",
    "$$\n",
    "\n",
    "When $p=\\infty$, Minkowski Distance can be expressed as Chebyshev Distance:\n",
    "$$\n",
    "d_{ij} = \\max|x_{ki}-x_{kj}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Mahalanobis Distance\n",
    "\n",
    "It's a clustering measure that considers the correlation between individual features. Given a set of samples $X=(x_{ij})_{m \\text{x} n}$ and its covariance matrix $S$, the Mahalanobis Distance between $x_{i}$ and $x_{j}$ can be defined as:\n",
    "$$\n",
    "d_{ij} = [(x_{i}-x_{j})^{T}S^{-1} (x_{i}-x_{j})]^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "When $S$ is an unit matrix, that is, when features of the sample are independent of each other and the variance is 1, Mahalanobis Distance is the same as Euclidean Distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Correlation Coefficient\n",
    "\n",
    "Correlation Coefficient is the most common way to measure similarity. The closer the correlation coefficient is to 1, the more similar the two samples are, and the closer the correlation coefficient is to 0, the less similar the two samples are. Correlation Coefficient between $x_{i}$ and $x_{j}$ can be defined as:\n",
    "$$\n",
    "s_{i j}=\\frac{\\sum_{k=1}^{m} x_{k i} x_{k j}}{\\left[\\sum_{k=1}^{m} x_{k i}^{2} \\sum_{k=1}^{m} x_{k j}^{2}\\right]^{\\frac{1}{2}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Cosine Similarity\n",
    "\n",
    "Cosine Similarity is also one of the ways to measure the similarity of two samples. The closer the cosine is to 1, the more similar the two samples are, and the closer the cosine is to 0, the less similar the two samples are. Cosine Similarity between $x_{i}$ and $x_{j}$ can be defined as:\n",
    "$$\n",
    "s_{i j} =\\cos (\\theta)=\\frac{\\sum_{k=1}^{m} x_{k i} x_{k j}}{\\left[\\sum_{k=1}^{m} x_{k i}^{2} \\sum_{k=1}^{m} x_{k j}^{2}\\right]^{\\frac{1}{2}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of samples $X=\\{x_{1},x_{2},\\cdot,x_{n}\\}$ with dimension of $m$ x $n$, k-means clustering is to divide n samples into k different categories ($k \\text{<} n$ in general). Therefore, k-means clustering can be summarized as the division of the sample set, and its learning strategy is to select the optimal division by minimizing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using Euclidean Distance to measure the distance between samples, the distance $d(x_{i},x_{j})$ is:\n",
    "$$\n",
    "d_{i j}=\\sum_{k=1}^{m}\\left(x_{k i}-x_{k j}\\right)^{2}=\\left\\|x_{i}-x_{j}\\right\\|^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define distance measurement\n",
    "def euclidean_distance(x1, x2):\n",
    "    distance = 0\n",
    "    for i in range(len(x1)):\n",
    "        distance += pow((x1[i] - x2[i]), 2)\n",
    "    return np.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the sum of the distances between the samples and their class centers as the loss function:\n",
    "$$\n",
    "W(C)=\\sum_{i=1}^{k} \\sum_{C(i)=l}\\left\\|x_{i}-\\bar{x}_{l}\\right\\|^{2}\n",
    "$$\n",
    "\n",
    "$\\bar{x}_{l}=\\left(\\bar{x}_{1 l}, \\bar{x}_{2 l}, \\ldots, \\bar{x}_{m l}\\right)^{T}$ is the center of $l$-th class. In $n_{l}=\\sum^{n}_{i=1}I(C(i)=l)$, $I(C(i)=l)$ is the indicator function, taking the value 1 or 0. The function $W(C)$ indicates how similar samples in the same class are. Hence, k-means cluster can be regarded as a solution for an optimization problem:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C^{*}&=\\arg \\min _{C} W(C) \\\\\n",
    "&=\\arg \\min _{C} \\sum_{l=1}^{k} \\sum_{C(i)=l}\\left\\|x_{i}-x_{j}\\right\\|^{2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps of the k-means clustering algorithm are as follows:\n",
    "\n",
    "• Initialize centers. That is, randomly select sample points as the initial cluster centre points at the $0$-th iteration: $m^{(0)}=(m_{1}^{(0)}, \\ldots, m_{l}^{(0)}, \\ldots, m_{k}^{(0)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the center points\n",
    "def centroids_init(k, X):\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = np.zeros((k, n_features))\n",
    "    for i in range(k):\n",
    "        # random choose a center point in each iteration\n",
    "        centroid = X[np.random.choice(range(n_samples))]\n",
    "        centroids[i] = centroid\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The samples are clustered according to their distance from the center. For the fixed centre points $m^{(t)}=(m_{1}^{(t)}, \\ldots, m_{l}^{(t)}, \\ldots, m_{k}^{(t)})$, calculate the distance from each sample to the center of the class, assign each sample to the class where its nearest center point is located, and form the preliminary clustering result $C^{(t)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the index of the nearest class center point to which each sample belongs\n",
    "def closest_centroid(sample, centroids):\n",
    "    closest_i = 0\n",
    "    closest_dist = float('inf')\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        # choose the class that the closet center point belongs to \n",
    "        distance = euclidean_distance(sample, centroid)\n",
    "        if distance < closest_dist:\n",
    "            closest_i = i\n",
    "            closest_dist = distance\n",
    "    return closest_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct different class\n",
    "def create_clusters(centroids, k, X):\n",
    "    n_samples = np.shape(X)[0]\n",
    "    clusters = [[] for _ in range(k)]\n",
    "    for sample_i, sample in enumerate(X):\n",
    "        # assign the sample to the closet class\n",
    "        centroid_i = closest_centroid(sample, centroids)\n",
    "        clusters[centroid_i].append(sample_i)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Calculate the new cluster center of the clustering result of the previous step. Calculate the current sample mean of each class for the clustering result $C^{(t)}$, and use it as the new class center $m^{(t+1)}=(m_{1}^{(t+1)}, \\ldots, m_{l}^{(t+1)}, \\ldots, m_{k}^{(t+1)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate the mean center point of each category based on the clustering results of the previous step\n",
    "def calculate_centroids(clusters, k, X):\n",
    "    n_features = np.shape(X)[1]\n",
    "    centroids = np.zeros((k, n_features))\n",
    "    # use the mean of all samples as the new center point\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        centroid = np.mean(X[cluster], axis=0)\n",
    "        centroids[i] = centroid\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• If the iteration converges or meets the iteration stop condition, output the final clustering result $C^{*}=C^{(t)}$, otherwise set $t=t+1$, and return to the second step to continue the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the class label of each sample\n",
    "def get_cluster_labels(clusters, X):\n",
    "    y_pred = np.zeros(np.shape(X)[0])\n",
    "    for cluster_i, cluster in enumerate(clusters):\n",
    "        for sample_i in cluster:\n",
    "            y_pred[sample_i] = cluster_i\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, max_iterations):\n",
    "    # step 1\n",
    "    centroids = centroids_init(k, X)\n",
    "    for _ in range(max_iterations): \n",
    "        # step 2\n",
    "        clusters = create_clusters(centroids, k, X) \n",
    "        prev_centroids = centroids\n",
    "        # step 3\n",
    "        centroids = calculate_centroids(clusters, k, X)\n",
    "        # step 4\n",
    "        diff = centroids - prev_centroids\n",
    "        if not diff.any():\n",
    "            break\n",
    "    \n",
    "    return get_cluster_labels(clusters, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,2],[0,0],[1,0],[5,0],[5,2]])\n",
    "labels = kmeans(X, 2, 10)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# kmeans in sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "print(kmeans.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
