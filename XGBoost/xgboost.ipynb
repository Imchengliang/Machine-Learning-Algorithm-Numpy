{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full name of XGBoost is eXtreme Gradient Boosting. And it is the same as GBDT that it belongs to ensemble learning algorithm but it has better performance in most scenario. \n",
    "\n",
    "Since XGBoost is similar with GBDT, XGBoost also is an additive model composed of multiple base models, so XGBoost can be expressed as:\n",
    "$$\n",
    "\\hat{y_{i}} = \\sum ^{K}_{k=1}f_{k}(x_{i})\n",
    "$$\n",
    "\n",
    "Assuming that the tree model needs to be trained in $t$-th iteration is $f_{t}(x)$, then:\n",
    "$$\n",
    "\\hat{y_{i}}^{(t)} = \\sum ^{t}_{k=1}\\hat{y_{i}}^{(t-1)} + f_{t}(x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original form of object function is:\n",
    "$$\n",
    "Obj = \\sum^{n}_{i=1}l(y_{i}, \\hat{y_{i}}) + \\sum^{t}_{i=1} \\Omega (f_{i})\n",
    "$$\n",
    "\n",
    "$\\sum^{t}_{i=1} \\Omega (f_{i})$ is the regularization term for the loss function, which represents the sum of the complexities of all t trees and aims to prevent the model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward stagewise algorithm is also used in XGBoost, using the model of $t$-th step as example, the prediction for the $i$-th sample $x_i$ is:\n",
    "$$\n",
    "\\hat{y_{i}}^{(t)} = \\hat{y_{i}}^{(t-1)} + f_{t}(x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $\\hat{y_{i}}^{(t-1)}$ is the prediction given on $t-1$-th step, it can be regarded as a known constant on $t$-th step, and $f_{t}(x_{i})$ is the tree model of $t$-th step. Meanwhile, the regularization term can also be split. Since the structure of the first $t-1$ trees has been determined, the sum of the complexities of the first $t-1$ trees can also be expressed as a constant. Then, the object function can be rewritten as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "O b j^{(t)} &=\\sum_{i=1}^{n} l\\left(y_{i}, \\hat{y}_{i}^{(t)}\\right)+\\sum_{i=1}^{t} \\Omega\\left(f_{i}\\right) \\\\\n",
    "&=\\sum_{i=1}^{n} l\\left(y_{i}, \\hat{y}_{i}^{(t-1)}+f_{t}\\left(x_{i}\\right)\\right)+\\Omega (f_{t})+\\sum_{i=1}^{t-1} \\Omega\\left(f_{i}\\right) \\\\\n",
    "&=\\sum_{i=1}^{n} l\\left(y_{i}, \\hat{y}_{i}^{(t-1)}+f_{t}\\left(x_{i}\\right)\\right)+\\Omega\\left(f_{t}\\right)+\\text { constant }\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the second-order Taylor formula, the loss function can be rewritten as:\n",
    "$$\n",
    "l\\left(y_{i}, \\hat{y}_{i}^{(t-1)}+f_{t}\\left(x_{i}\\right)\\right)=l\\left(y_{i}, \\hat{y}_{i}^{(t-1)}\\right)+g_{i} f_{t}\\left(x_{i}\\right)+\\frac{1}{2} h_{i} f_{t}^{2}\\left(x_{i}\\right)\n",
    "$$\n",
    "\n",
    "$g_{i}$ is the first derivative of the loss function, $h_{i}$ is the second derivative of the loss function. XGBoost uses the second derivative information, so if custom loss function is used, its second derivation has to be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using square loss function as an example:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&l(y_{i}, \\hat{y}_{i}^{(t-1)}) = (y_{i}-\\hat{y}_{i}^{(t-1)})^{2} \\\\\n",
    "&g_{i}=\\frac{\\partial l\\left(y_{i}, \\hat{y}_{i}^{(t-1)}\\right)}{\\partial \\hat{y}_{i}^{(t-1)}}=-2\\left(y_{i}-\\hat{y}_{i}^{(t-1)}\\right)\\\\\n",
    "&h_{i}=\\frac{\\partial^{2} l\\left(y_{i}, \\hat{y}_{i}^{(t-1)}\\right)}{\\partial\\left(\\hat{y}_{i}^{(t-1)}\\right)^{2}}=2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing this second-order Taylor expansion into the XGBoost object function derived above, the approximate expression of the object function can be obtained:\n",
    "$$\n",
    "O b j^{(t)} \\simeq \\sum_{i=1}^{n}\\left[l\\left(y_{i}, \\hat{y}_{i}^{t-1}\\right)+g_{i} f_{t}\\left(x_{i}\\right)+\\frac{1}{2} h_{i} f_{t}^{2}\\left(x_{i}\\right)\\right]+\\Omega\\left(f_{t}\\right)+\\text { constant }\n",
    "$$\n",
    "\n",
    "Removing the relevant constant term from the above formula, the simplified object function is:\n",
    "$$\n",
    "O b j^{(t)} \\simeq \\sum_{i=1}^{n}\\left[g_{i} f_{t}\\left(x_{i}\\right)+\\frac{1}{2} h_{i} f_{t}^{2}\\left(x_{i}\\right)\\right]+\\Omega\\left(f_{t}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is only necessary to solve the first-order derivative and the second-order derivative of the loss function of each step, and then optimize the objective function to obtain the $f(x)$ of each step, then a boosting model can be obtained according to the addition model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two significant components in a decision tree, weight vector of leaf nodes $w$, and the mapping relationship between instances and leaf nodes $q$. So the mathematical expression of a tree is:\n",
    "$$\n",
    "f_{t}(X) = w_{q(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the regularization term for model complexity, the model complexity $\\Omega$ can be determined by the number of leaf nodes $T$ and the weight of leaf $w$. Specifically, the complexity of the loss function is determined by the number of leaf nodes and leaf weights of all trees. The mathematical expression is as follows:\n",
    "$$\n",
    "\\Omega(f_{t}) = \\gamma T + \\frac{1}{2}\\lambda \\sum^{T}_{i=1}w^{2}_{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then regrouping all leaf nodes, which is allocating all samples $x_i$ belonging to the $j$-th leaf node into the sample set of one leaf node, that is: $I_{j}=\\{i|q(x_{i})=j \\}$. And the object function can be written as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "O b j^{(t)} & \\simeq \\sum_{i=1}^{n}\\left[g_{i} f_{t}\\left(x_{i}\\right)+\\frac{1}{2} h_{i} f_{t}^{2}\\left(x_{i}\\right)\\right]+\\Omega\\left(f_{t}\\right) \\\\\n",
    "&=\\sum_{i=1}^{n}\\left[g_{i} w_{q\\left(x_{i}\\right)}+\\frac{1}{2} h_{i} w_{q\\left(x_{i}\\right)}^{2}\\right]+\\gamma T+\\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_{j}^{2} \\\\\n",
    "&=\\sum_{j=1}^{T}\\left[\\left(\\sum_{i \\in I_{j}} g_{i}\\right) w_{j}+\\frac{1}{2}\\left(\\sum_{i \\in I_{j}} h_{i}+\\lambda\\right) w_{j}^{2}\\right]+\\gamma T\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $G_{j}=\\sum _{i \\in I_{j}}g_{i}$ and $H_{j}=\\sum _{i \\in I_{j}}h_{i}$: \n",
    "\n",
    "$ \\enspace \\text{•} \\enspace G_{j}$: The accumulated sum of the first-order partial derivatives of the samples contained in the leaf node j, it is a constant. \n",
    "\n",
    "$ \\enspace \\text{•} \\enspace H_{j}$: The accumulated sum of the second-order partial derivatives of the samples contained in the leaf node j, it is a constant.\n",
    "\n",
    "Put $G_{j}$ and $H_{j}$ into the object function above, the final version of object function for XGBoost is as follow:\n",
    "$$\n",
    "Obj^{(t)}=\\sum_{j=1}^{T}\\left[G_{j} w_{j}+\\frac{1}{2}(H_{j}+\\lambda) w_{j}^{2}\\right]+\\gamma T\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the solution formula of one-dimensional quadratic equation, the following are obtained:\n",
    "$$\n",
    "\\begin{gathered}\n",
    "x^{*}=-\\frac{b}{2 a}=\\frac{G}{H} \\\\\n",
    "y^{*}=\\frac{4 a c-b^{2}}{4 a}=-\\frac{G^{2}}{2 H}\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disassemble each leaf node $j$ from the objective function, there is:\n",
    "$$\n",
    "G_{j} w_{j}+\\frac{1}{2}(H_{j}+\\lambda) w_{j}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the above derivation that $G_{j}$ and $H_{j}$ in the $t$-th tree can be calculated. Therefore, this formula is a one-variable quadratic function that contains only one variable, leaf node weight $w$, and its maximum point can be calculated according to the maximum value formula. When the leaf nodes of each independent tree reach the optimal value, the entire loss function also reaches the optimal status accordingly.\n",
    "\n",
    "When the structure of a tree is fixed and let the formula above be equal to 0, the optimal point and optimal value are:\n",
    "$$\n",
    "\\begin{gathered}\n",
    "w_{j}^{*}=-\\frac{G_{j}}{H_{j}+\\lambda} \\\\\n",
    "O b j=-\\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_{j}^{2}}{H_{j}+\\lambda}+\\gamma T\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with GBDT, XGBoost mainly has difference on information divergence calculation, leaf computation, and the use of second-order derivative of the loss function.\n",
    "\n",
    "According to the second-order derivative information, the loss function of XGBoost is optimized to a state that is very close to the real loss. Its node splitting method is not essentially different from the node splitting method of the CART tree, but the calculation of the information divergence is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the model processes feature splitting on one node, the object function before splitting is:\n",
    "$$\n",
    "Obj_{1} = - \\frac{1}{2}[\\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+ \\lambda}]+ \\gamma\n",
    "$$\n",
    "\n",
    "The object function after splitting is:\n",
    "$$\n",
    "Obj_{1} = - \\frac{1}{2}[\\frac{G_{L}^{2}}{H_{L}+ \\lambda}+\\frac{G_{R}^{2}}{H_{R}+ \\lambda}]+ 2 \\gamma\n",
    "$$\n",
    "\n",
    "The information divergence after splitting is:\n",
    "$$\n",
    "Gain = \\frac{1}{2}[\\frac{G_{L}^{2}}{H_{L}+ \\lambda}+\\frac{G_{R}^{2}}{H_{R}+ \\lambda}-\\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+ \\lambda}]- \\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from CART import TreeNode, BinaryDecisionTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import cat_label_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostTree(BinaryDecisionTree):\n",
    "    def _split(self, y):\n",
    "        col = int(np.shape(y)[1]/2)\n",
    "        y, y_pred = y[:, :col], y[:, col:]\n",
    "        return y, y_pred\n",
    "\n",
    "    # calculate information divergence\n",
    "    def _gain(self, y, y_pred):\n",
    "        Gradient = np.power((y * self.loss.gradient(y, y_pred)).sum(), 2) \n",
    "        Hessian = self.loss.hess(y, y_pred).sum()\n",
    "        return 0.5 * (Gradient / Hessian)\n",
    "\n",
    "    # calculate divergence in tree splitting\n",
    "    def _gain_by_taylor(self, y, y1, y2):\n",
    "        # node split\n",
    "        y, y_pred = self._split(y)\n",
    "        y1, y1_pred = self._split(y1)\n",
    "        y2, y2_pred = self._split(y2)\n",
    "\n",
    "        true_gain = self._gain(y1, y1_pred)\n",
    "        false_gain = self._gain(y2, y2_pred)\n",
    "        gain = self._gain(y, y_pred)\n",
    "        return true_gain + false_gain - gain\n",
    "\n",
    "    # find the optimized weight for leaf node\n",
    "    def _approximate_update(self, y):\n",
    "        y, y_pred = self._split(y)\n",
    "        # Newton's method\n",
    "        gradient = np.sum(y * self.loss.gradient(y, y_pred), axis=0)\n",
    "        hessian = np.sum(self.loss.hess(y, y_pred), axis=0) \n",
    "        update_approximation = gradient / hessian\n",
    "        return update_approximation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._gain_by_taylor\n",
    "        self._leaf_value_calculation = self._approximate_update\n",
    "        super(XGBoostTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for classification\n",
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "class LogLoss:\n",
    "    def __init__(self):\n",
    "        sigmoid = Sigmoid()\n",
    "        self._func = sigmoid\n",
    "        self._grad = sigmoid.gradient\n",
    "    \n",
    "    # define loss function\n",
    "    def loss(self, y, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        p = self._func(y_pred)\n",
    "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
    "\n",
    "    # first-order derivative\n",
    "    def gradient(self, y, y_pred):\n",
    "        p = self._func(y_pred)\n",
    "        return -(y - p)\n",
    "\n",
    "    # second-order derivative\n",
    "    def hess(self, y, y_pred):\n",
    "        p = self._func(y_pred)\n",
    "        return p * (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGBoost model based on forward step algorithm\n",
    "class XGBoost:\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.001, min_samples_split=2, min_gini_impurity=999, max_depth=2):\n",
    "        # number of tree\n",
    "        self.n_estimators = n_estimators\n",
    "        # step size for weight update\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_gini_impurity = min_gini_impurity\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # square loss for regression\n",
    "        # self.loss = SquaresLoss()\n",
    "        # logarithmic loss for classification\n",
    "        self.loss = LogLoss()\n",
    "        # initialize the list for classification tree\n",
    "        self.trees = []\n",
    "        # build the decision tree in iteration\n",
    "        for _ in range(n_estimators):\n",
    "            tree = XGBoostTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    min_gini_impurity=self.min_gini_impurity,\n",
    "                    max_depth=self.max_depth,\n",
    "                    loss=self.loss)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = cat_label_convert(y)\n",
    "        y_pred = np.zeros(np.shape(y))\n",
    "        # accumulate results after fitting each tree\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = self.trees[i]\n",
    "            y_true_pred = np.concatenate((y, y_pred), axis=1)\n",
    "            tree.fit(X, y_true_pred)\n",
    "            iter_pred = tree.predict(X)\n",
    "            y_pred -= np.multiply(self.learning_rate, iter_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = None\n",
    "        # prediction in iteration\n",
    "        for tree in self.trees:\n",
    "            iter_pred = tree.predict(X)\n",
    "            if y_pred is None:\n",
    "                y_pred = np.zeros_like(iter_pred)\n",
    "            y_pred -= np.multiply(self.learning_rate, iter_pred)\n",
    "        y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)\n",
    "        # transform the prediction into label\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imchengliang/Downloads/Code/ML/XGBoost/CART.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([X_left, X_right])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m43\u001b[39m)  \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m clf \u001b[39m=\u001b[39m XGBoost()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "\u001b[1;32m/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb Cell 23\u001b[0m in \u001b[0;36mXGBoost.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m tree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees[i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m y_true_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((y, y_pred), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m tree\u001b[39m.\u001b[39;49mfit(X, y_true_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m iter_pred \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mpredict(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_pred \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmultiply(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate, iter_pred)\n",
      "\u001b[1;32m/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb Cell 23\u001b[0m in \u001b[0;36mXGBoostTree.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impurity_calculation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gain_by_taylor\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_leaf_value_calculation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_approximate_update\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imchengliang/Downloads/Code/ML/XGBoost/xgboost.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39msuper\u001b[39;49m(XGBoostTree, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m~/Downloads/Code/ML/XGBoost/CART.py:71\u001b[0m, in \u001b[0;36mBinaryDecisionTree.fit\u001b[0;34m(self, X, y, loss)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     70\u001b[0m     \u001b[39m# build the decision tree recursively\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_tree(X, y)\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Code/ML/XGBoost/CART.py:110\u001b[0m, in \u001b[0;36mBinaryDecisionTree._build_tree\u001b[0;34m(self, X, y, current_depth)\u001b[0m\n\u001b[1;32m    106\u001b[0m y2 \u001b[39m=\u001b[39m Xy2[:, n_features:]\n\u001b[1;32m    108\u001b[0m \u001b[39m# calculate gini impurity\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# impurity = self.impurity_calculation(y, y1, y2)\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m impurity \u001b[39m=\u001b[39m calculate_gini(y)\n\u001b[1;32m    112\u001b[0m \u001b[39m# update minimum gini impurity, feature index and\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m impurity \u001b[39m<\u001b[39m init_gini_impurity:\n",
      "File \u001b[0;32m~/Downloads/Code/ML/XGBoost/CART.py:19\u001b[0m, in \u001b[0;36mcalculate_gini\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_gini\u001b[39m(y):\n\u001b[1;32m     18\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> 19\u001b[0m     probs \u001b[39m=\u001b[39m [y\u001b[39m.\u001b[39mcount(i)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(y) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39munique(y)]\n\u001b[1;32m     20\u001b[0m     gini \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([p\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mp) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m probs])\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m gini\n",
      "File \u001b[0;32m~/Downloads/Code/ML/XGBoost/CART.py:19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_gini\u001b[39m(y):\n\u001b[1;32m     18\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> 19\u001b[0m     probs \u001b[39m=\u001b[39m [y\u001b[39m.\u001b[39;49mcount(i)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(y) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39munique(y)]\n\u001b[1;32m     20\u001b[0m     gini \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([p\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mp) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m probs])\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m gini\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)  \n",
    "clf = XGBoost()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',   \n",
    "    'num_class': 3,     \n",
    "    'gamma': 0.1,\n",
    "    'max_depth': 2,\n",
    "    'lambda': 2,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 3,\n",
    "    'eta': 0.001,\n",
    "    'seed': 1000,\n",
    "    'nthread': 4,\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "num_rounds = 200\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy:\", accuracy)\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
